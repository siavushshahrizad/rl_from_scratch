# Introduction
This repository tests the [vanilla policy gradient](https://link.springer.com/article/10.1007/BF00992696) (VPG) algorithm - also called REINFORCE - and the [Proximal Policy Optimisation](https://arxiv.org/pdf/1707.06347) (PPO) on different environments in the OpenAI Gymnasiumn API. My goal was to document the strengths and weaknesses of the algorithms as well as the lessons I had learned when implementing it. Having embarked on my RL journey, I found that empirical and implementation insights were lacking, and I was hoping that this material would be useful for both me and other researchers. An important reason why I created this repository is because research indicated that [simple algorithms could outperform more complex ones](https://arxiv.org/abs/2005.12729) because how [algorithms were implemented mattered](https://arxiv.org/abs/2006.05990). 

# How to run this repository locally

# Performance lessons

# Implementation lessons

# Conclusion
