# Introduction
This repository tests the vanilla policy gradient (VPG) algorithm - also called REINFORCE - and the Proximal Policy Optimisation (PPO) on different environments in the OpenAI Gymnasiumn API. My goal was to document the strengths and weaknesses of the algorithms as well as the lessons I had learned when implementing it. Having embarked on my RL journey, I found that empirical and implementation insights were lacking, and I was hoping that this material would be useful for both me and other researchers. An important reason why I created this repository is because research indicated that [simple algorithm could outperform more complex](https://arxiv.org/abs/2005.12729) and supposedly better algorithms such as PPO because how [algorithms were implemented mattered](https://arxiv.org/abs/2006.05990).

# How to run this repository locally

# Performance lessons

# Implementationa lessons

# Conclusion
